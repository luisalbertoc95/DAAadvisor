#!/usr/bin/env python3
"""
Advanced metadata testing: longitudinal, disease states, complex designs
"""

import sys
sys.path.insert(0, '.')

import pandas as pd
import numpy as np
from daa_advisor import DifferentialAbundanceTool
from daa_advisor.methods.registry import MethodRegistry
from daa_advisor.data_generators import MicrobiomeDataGenerator
import time
from pathlib import Path

def create_longitudinal_dataset():
    """Create pre/post treatment longitudinal dataset"""
    
    np.random.seed(123)
    n_patients = 30
    n_features = 200
    n_differential = 25
    
    # Generate patient IDs
    patient_ids = [f"Patient_{i:02d}" for i in range(1, n_patients + 1)]
    
    # Create pre/post samples for each patient
    sample_names = []
    timepoints = []
    treatments = []
    patient_mapping = []
    
    for pid in patient_ids:
        # Pre-treatment sample
        sample_names.append(f"{pid}_Pre")
        timepoints.append("Pre")
        treatments.append("Baseline")
        patient_mapping.append(pid)
        
        # Post-treatment sample
        sample_names.append(f"{pid}_Post")
        timepoints.append("Post")
        treatments.append("Treatment")
        patient_mapping.append(pid)
    
    n_samples = len(sample_names)
    
    # Generate baseline microbiome data
    baseline = np.random.negative_binomial(20, 0.3, (n_samples, n_features)) + 5
    
    # Add treatment effect to post-treatment samples
    post_mask = np.array(timepoints) == "Post"
    differential_features = np.random.choice(n_features, n_differential, replace=False)
    
    for feat_idx in differential_features:
        # Some features increase, some decrease with treatment
        if feat_idx % 2 == 0:
            # Increase
            multiplier = np.random.uniform(2.5, 4.0)
            baseline[post_mask, feat_idx] = (baseline[post_mask, feat_idx] * multiplier).astype(int)
        else:
            # Decrease
            multiplier = np.random.uniform(0.2, 0.5)
            baseline[post_mask, feat_idx] = (baseline[post_mask, feat_idx] * multiplier).astype(int)
    
    # Create DataFrames
    count_table = pd.DataFrame(
        baseline.astype(int),
        columns=[f'ASV_{i:03d}' for i in range(n_features)],
        index=sample_names
    )
    
    metadata = pd.DataFrame({
        'timepoint': timepoints,
        'treatment': treatments,
        'patient_id': patient_mapping,
        'collection_week': [0 if tp == "Pre" else 8 for tp in timepoints],
        'batch': np.random.choice(['Batch_A', 'Batch_B', 'Batch_C'], n_samples)
    }, index=sample_names)
    
    true_features = [f'ASV_{i:03d}' for i in differential_features]
    
    return count_table, metadata, true_features

def create_disease_progression_dataset():
    """Create healthy -> disease -> recovery dataset"""
    
    np.random.seed(456)
    n_subjects = 40
    n_features = 300
    n_differential = 30
    
    # Create disease progression states
    states = ['Healthy', 'Disease', 'Recovery']
    sample_names = []
    disease_states = []
    subject_ids = []
    severity_scores = []
    
    for subj_id in range(1, n_subjects + 1):
        for state in states:
            sample_names.append(f"Subject_{subj_id:02d}_{state}")
            disease_states.append(state)
            subject_ids.append(f"Subject_{subj_id:02d}")
            
            # Add severity scores
            if state == 'Healthy':
                severity_scores.append(0)
            elif state == 'Disease':
                severity_scores.append(np.random.randint(5, 10))
            else:  # Recovery
                severity_scores.append(np.random.randint(1, 4))
    
    n_samples = len(sample_names)
    
    # Generate baseline microbiome
    baseline = np.random.negative_binomial(15, 0.25, (n_samples, n_features)) + 3
    
    # Add disease effects
    disease_mask = np.array(disease_states) == "Disease"
    recovery_mask = np.array(disease_states) == "Recovery"
    differential_features = np.random.choice(n_features, n_differential, replace=False)
    
    for feat_idx in differential_features:
        # Disease state changes
        disease_multiplier = np.random.uniform(0.1, 0.3) if feat_idx % 3 == 0 else np.random.uniform(3.0, 6.0)
        baseline[disease_mask, feat_idx] = (baseline[disease_mask, feat_idx] * disease_multiplier).astype(int)
        
        # Recovery state (partial restoration)
        recovery_multiplier = np.random.uniform(0.5, 1.5)
        baseline[recovery_mask, feat_idx] = (baseline[recovery_mask, feat_idx] * recovery_multiplier).astype(int)
    
    # Create DataFrames
    count_table = pd.DataFrame(
        baseline.astype(int),
        columns=[f'Taxa_{i:03d}' for i in range(n_features)],
        index=sample_names
    )
    
    metadata = pd.DataFrame({
        'disease_state': disease_states,
        'subject_id': subject_ids,
        'severity_score': severity_scores,
        'age': np.random.randint(25, 75, n_samples),
        'bmi': np.random.normal(25, 4, n_samples),
        'medication': np.random.choice(['None', 'Antibiotic', 'Probiotic'], n_samples)
    }, index=sample_names)
    
    true_features = [f'Taxa_{i:03d}' for i in differential_features]
    
    return count_table, metadata, true_features

def create_multifactorial_dataset():
    """Create dataset with multiple interacting factors"""
    
    np.random.seed(789)
    n_samples = 120
    n_features = 250
    n_differential = 35
    
    # Generate multiple factors
    sample_names = [f"Sample_{i:03d}" for i in range(1, n_samples + 1)]
    
    # Main factors
    conditions = np.random.choice(['Control', 'Treatment_A', 'Treatment_B'], n_samples)
    genders = np.random.choice(['Male', 'Female'], n_samples)
    age_groups = np.random.choice(['Young', 'Middle', 'Old'], n_samples)
    batches = np.random.choice(['Batch_1', 'Batch_2', 'Batch_3', 'Batch_4'], n_samples)
    
    # Generate baseline data
    baseline = np.random.negative_binomial(18, 0.28, (n_samples, n_features)) + 4
    
    # Add complex interaction effects
    differential_features = np.random.choice(n_features, n_differential, replace=False)
    
    for feat_idx in differential_features:
        # Treatment effects
        treatment_a_mask = conditions == 'Treatment_A'
        treatment_b_mask = conditions == 'Treatment_B'
        
        # Gender-specific effects
        male_mask = genders == 'Male'
        female_mask = genders == 'Female'
        
        # Age-specific effects
        young_mask = age_groups == 'Young'
        old_mask = age_groups == 'Old'
        
        # Complex interactions
        if feat_idx % 3 == 0:
            # Treatment A + Male interaction
            interaction_mask = treatment_a_mask & male_mask
            baseline[interaction_mask, feat_idx] = (baseline[interaction_mask, feat_idx] * np.random.uniform(2.0, 4.0)).astype(int)
        elif feat_idx % 3 == 1:
            # Treatment B + Female + Young interaction
            interaction_mask = treatment_b_mask & female_mask & young_mask
            baseline[interaction_mask, feat_idx] = (baseline[interaction_mask, feat_idx] * np.random.uniform(3.0, 5.0)).astype(int)
        else:
            # Age effect regardless of treatment
            baseline[old_mask, feat_idx] = (baseline[old_mask, feat_idx] * np.random.uniform(0.3, 0.7)).astype(int)
    
    # Create DataFrames
    count_table = pd.DataFrame(
        baseline.astype(int),
        columns=[f'Feature_{i:03d}' for i in range(n_features)],
        index=sample_names
    )
    
    metadata = pd.DataFrame({
        'condition': conditions,
        'gender': genders,
        'age_group': age_groups,
        'batch': batches,
        'age_numeric': np.random.randint(18, 80, n_samples),
        'bmi': np.random.normal(24, 5, n_samples),
        'smoking': np.random.choice(['Never', 'Former', 'Current'], n_samples)
    }, index=sample_names)
    
    true_features = [f'Feature_{i:03d}' for i in differential_features]
    
    return count_table, metadata, true_features

def test_metadata_scenario(dataset_name, count_table, metadata, true_features, group_column):
    """Test a single metadata scenario"""
    
    print(f"\nğŸ”¬ Testing: {dataset_name}")
    print("-" * 60)
    print(f"ğŸ“Š Data: {count_table.shape[0]} samples Ã— {count_table.shape[1]} features")
    print(f"ğŸ¯ True differential: {len(true_features)}")
    print(f"ğŸ‘¥ Groups: {metadata[group_column].value_counts().to_dict()}")
    
    # Test individual methods
    registry = MethodRegistry()
    methods_to_test = ['wilcoxon', 'deseq2', 'edger', 'aldex2']  # Fastest methods for comprehensive testing
    
    results = {}
    
    for method_name in methods_to_test:
        print(f"  Testing {method_name}...", end=" ")
        try:
            start_time = time.time()
            method = registry.get_method(method_name)
            result = method.run(count_table, metadata, group_column=group_column)
            runtime = time.time() - start_time
            
            # Calculate performance
            significant = result[result['padj'] < 0.05]
            sig_features = set(significant['feature'].tolist())
            true_set = set(true_features)
            
            tp = len(true_set & sig_features)
            fp = len(sig_features - true_set)
            fn = len(true_set - sig_features)
            
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
            
            results[method_name] = {
                'success': True,
                'significant': len(significant),
                'f1': f1,
                'precision': precision,
                'recall': recall,
                'runtime': runtime
            }
            
            print(f"âœ… {len(significant)} sig, F1={f1:.3f}")
            
        except Exception as e:
            results[method_name] = {'success': False, 'error': str(e)[:50]}
            print(f"âŒ FAILED")
    
    # Test consensus
    print("  Testing consensus...", end=" ")
    try:
        start_time = time.time()
        tool = DifferentialAbundanceTool()
        consensus_results = tool.analyze(
            count_table=count_table,
            metadata=metadata,
            data_type='asv',
            use_consensus=True,
            group_column=group_column
        )
        consensus_runtime = time.time() - start_time
        
        consensus_features = tool.get_significant_features(alpha=0.05)
        
        if hasattr(consensus_features, 'feature'):
            consensus_set = set(consensus_features['feature'])
        elif isinstance(consensus_features, list):
            consensus_set = set(consensus_features)
        else:
            consensus_set = set()
        
        true_set = set(true_features)
        tp = len(true_set & consensus_set)
        fp = len(consensus_set - true_set)
        fn = len(true_set - consensus_set)
        
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        
        results['consensus'] = {
            'success': True,
            'significant': len(consensus_set),
            'f1': f1,
            'precision': precision,
            'recall': recall,
            'runtime': consensus_runtime
        }
        
        print(f"âœ… {len(consensus_set)} sig, F1={f1:.3f}")
        
    except Exception as e:
        results['consensus'] = {'success': False, 'error': str(e)[:50]}
        print(f"âŒ FAILED")
    
    return results

def run_advanced_metadata_testing():
    """Run comprehensive metadata testing"""
    
    print("ğŸ§¬ ADVANCED METADATA TESTING FRAMEWORK")
    print("=" * 60)
    print("Testing complex experimental designs and metadata types")
    print()
    
    # Create output directory
    output_dir = "advanced_metadata_results"
    Path(output_dir).mkdir(exist_ok=True)
    
    all_results = {}
    
    # 1. Longitudinal (Pre/Post) Analysis
    print("ğŸ“Š Generating longitudinal dataset...")
    long_counts, long_meta, long_true = create_longitudinal_dataset()
    long_results = test_metadata_scenario(
        "Longitudinal Pre/Post Treatment", 
        long_counts, long_meta, long_true, 
        'timepoint'
    )
    all_results['longitudinal'] = long_results
    
    # 2. Disease Progression Analysis
    print("\nğŸ“Š Generating disease progression dataset...")
    disease_counts, disease_meta, disease_true = create_disease_progression_dataset()
    disease_results = test_metadata_scenario(
        "Disease Progression (Healthy/Disease/Recovery)", 
        disease_counts, disease_meta, disease_true, 
        'disease_state'
    )
    all_results['disease_progression'] = disease_results
    
    # 3. Multi-factorial Analysis
    print("\nğŸ“Š Generating multi-factorial dataset...")
    multi_counts, multi_meta, multi_true = create_multifactorial_dataset()
    multi_results = test_metadata_scenario(
        "Multi-factorial Design (Treatment Ã— Gender Ã— Age)", 
        multi_counts, multi_meta, multi_true, 
        'condition'
    )
    all_results['multifactorial'] = multi_results
    
    # Generate summary
    print("\n" + "=" * 60)
    print("ğŸ“‹ ADVANCED METADATA TESTING SUMMARY")
    print("=" * 60)
    
    scenarios = ['longitudinal', 'disease_progression', 'multifactorial']
    methods = ['wilcoxon', 'deseq2', 'edger', 'aldex2', 'consensus']
    
    print(f"\nğŸ“Š Success Rate by Scenario:")
    for scenario in scenarios:
        results = all_results[scenario]
        successful = sum(1 for r in results.values() if r['success'])
        total = len(results)
        print(f"  {scenario.replace('_', ' ').title()}: {successful}/{total} ({successful/total*100:.1f}%)")
    
    print(f"\nğŸ† Best Performers by Scenario:")
    for scenario in scenarios:
        results = all_results[scenario]
        working_methods = {m: r for m, r in results.items() if r['success']}
        if working_methods:
            best_method = max(working_methods.keys(), key=lambda m: working_methods[m]['f1'])
            best_f1 = working_methods[best_method]['f1']
            print(f"  {scenario.replace('_', ' ').title()}: {best_method.upper()} (F1={best_f1:.3f})")
    
    print(f"\nâœ… Advanced metadata testing complete!")
    print(f"ğŸ“ Results demonstrate DAAadvisor's capability to handle:")
    print(f"  â€¢ Longitudinal/paired sample designs")
    print(f"  â€¢ Disease progression studies")
    print(f"  â€¢ Multi-factorial experimental designs")
    print(f"  â€¢ Complex metadata interactions")
    
    return all_results

if __name__ == "__main__":
    results = run_advanced_metadata_testing()
    print(f"\nğŸ¯ DAAadvisor successfully validated across diverse metadata types!")