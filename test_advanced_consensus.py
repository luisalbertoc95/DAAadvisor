#!/usr/bin/env python3
"""
Test script for advanced consensus analysis integration
"""

import numpy as np
import pandas as pd
from daa_advisor import DifferentialAbundanceTool, MicrobiomeDataGenerator

def test_advanced_consensus():
    """Test the integrated advanced consensus analysis"""
    
    print("ğŸ§ª Testing Advanced Consensus Analysis Integration")
    print("=" * 60)
    
    # Generate test data
    generator = MicrobiomeDataGenerator()
    count_table, metadata, differential_features = generator.generate_asv_data(
        n_samples=50,
        n_features=100,
        effect_size=2.0,
        n_differential=20
    )
    
    print(f"ğŸ“Š Generated test data: {count_table.shape} samples x features")
    print(f"ğŸ¯ {metadata['condition'].value_counts().to_dict()} samples per group")
    print(f"ğŸ”¬ {len(differential_features)} differential features generated")
    
    # Run analysis with consensus
    tool = DifferentialAbundanceTool()
    results = tool.analyze(
        count_table=count_table,
        metadata=metadata,
        data_type='asv',
        use_consensus=True
    )
    
    print(f"\nğŸ”¬ Methods run: {list(results['analyses'].keys())}")
    
    # Test consensus results
    if 'consensus' in results:
        consensus = results['consensus']
        print(f"\nğŸ¤ Advanced Consensus Results:")
        print(f"   â€¢ Total features analyzed: {len(consensus)}")
        print(f"   â€¢ Consensus significant: {consensus['consensus_significant'].sum()}")
        
        if 'confidence_score' in consensus.columns:
            print(f"   â€¢ High confidence calls: {consensus['high_confidence'].sum()}")
            print(f"   â€¢ Mean confidence score: {consensus['confidence_score'].mean():.3f}")
        
        if 'consensus_strength' in consensus.columns:
            strength_dist = consensus['consensus_strength'].value_counts()
            print(f"   â€¢ Consensus strength distribution:")
            for strength, count in strength_dist.items():
                print(f"     - {strength}: {count}")
    
    # Test agreement metrics
    if 'consensus_metrics' in results:
        metrics = results['consensus_metrics']
        print(f"\nğŸ“ˆ Agreement Metrics:")
        
        if 'mean_kappa' in metrics:
            print(f"   â€¢ Mean Cohen's Îº: {metrics['mean_kappa']:.3f} ({metrics['kappa_interpretation']})")
        
        if 'majority_agreement' in metrics:
            print(f"   â€¢ Majority agreement: {metrics['majority_agreement']:.1%}")
        
        if 'unanimous_agreement' in metrics:
            print(f"   â€¢ Unanimous agreement: {metrics['unanimous_agreement']:.1%}")
        
        if 'method_agreement' in metrics:
            print(f"   â€¢ Method-specific agreement:")
            for method, agreement in metrics['method_agreement'].items():
                print(f"     - {method}: {agreement:.3f}")
    
    # Display summary
    print(f"\nğŸ“‹ Analysis Summary:")
    tool.summarize_results()
    
    print(f"\nâœ… Advanced consensus analysis integration test completed!")
    return results

if __name__ == "__main__":
    test_advanced_consensus()